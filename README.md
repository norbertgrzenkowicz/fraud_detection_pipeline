# Fraud Detection System

## Overview
A real-time fraud detection system that processes credit card transactions through a streaming pipeline using Apache Kafka, detects potential fraud using machine learning models, and provides API endpoints for predictions.

## Architecture
```plaintext
                   ┌───────────────┐
                   │ Data Producer │
                   └──────┬────────┘
                          │
                          ▼
                    ┌──────────┐
                    │   Kafka   │
                    └────┬─────┘
                          │
                          ▼
                   ┌────────────┐
                   │   Airflow  │
                   │    DAGs    │
                   └────┬───────┘
                          │
                          ▼
                  ┌────────────────┐
                  │  Postgres DB   │
                  └──────┬─────────┘
                          │
         ┌────────────────┴─────────────┐
         ▼                              ▼
┌──────────────┐              ┌────────────────┐
│ Relearning   │              │   REST API     │
│   Model      │              │   Service      │
└──────────────┘              └────────────────┘
```

## Features
- Real-time transaction processing using Apache Kafka
- Machine learning-based fraud detection
- RESTful API for making predictions
- Automated data pipeline using Apache Airflow
- PostgreSQL database for transaction storage
- Docker containerization for all components

## Prerequisites
- Docker and Docker Compose
- Python 3.8+
- Apache Airflow
- Apache Kafka
- PostgreSQL

## Installation

1. Clone the repository
2. Make sure you have docker installed
3. Start the services using Docker Compose in root folder
```bash
docker-compose up -d
```

## Usage
1. After starting docker services you'll have running Apache Kafka, Airflow, PostgreSQL instances and Data generation script ran once.
2. head over to `localhost:8090` and login with admin:admin to get to Airflow view.
3. Run `consume_dag` to consume messages in kafka generated by `synthetic_data.py` and sent to topic `fraud`. Consumed transactions will be served by model, with given prediction sent to credit-card database to `transactions` table.
4. Run `model_service_update` to fetch data from psql which will be used by `model/src/train.py` to train the `lr_model.pkl`. At the end DAG will push created model with FastAPI to Docker Image.
5. Start API image by typing `docker run norbertgrzenkowicz/fraud-api:latest`.
6. Head over to localhost:8100 to check on FastAPI UI or create a request to get a prediction from `lr_model.pkl`.
```bash
curl -X POST "http://localhost:8100/predict"      -H "Content-Type: application/json"      -d '{"Time": 151286.0, "V1": -1.283143795718, "V2": -0.844000117728, "V3": 2.568569206224, "V4": -0.499431313524, "V5": 0.0753872465, "V6": -1.270077366353, "V7": 0.394572286331, "V8": -1.081260469094, "V9": 1.185694517993, "V10": -0.554127684353, "V11": -0.326845521923, "V12": 1.843859625707, "V13": 0.545676239052, "V14": 0.399648534239, "V15": 0.133206149608, "V16": -0.005156703292, "V17": 1.889110374068, "V18": 0.132439305377, "V19": 0.168066572872, "V20": 0.766148523442, "V21": -1.323289017777, "V22": -0.511857308692, "V23": -0.99375223135, "V24": -0.350672043067, "V25": 0.837437623982, "V26": -0.830720233847, "V27": -0.895553490449, "V28": 1.07607051852, "Amount": 0.788034601603}'
```

### Accessing the Services
- Airflow UI: localhost:8090
- Kafka: localhost:9092
- PostgreSQL: localhost:5433
- REST API: localhost:8100

### Making Predictions
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"amount": 100.0, "time": 0, ...}'
```

## Development(AI Generated README down there)

### Running Tests(None atm)
```bash
pytest
```

### Adding New Features
1. Create a new branch
2. Make your changes
3. Write tests
4. Submit a pull request

## Monitoring
- Airflow DAG status: Check Airflow UI
- API health: `/health` endpoint
- Kafka topics: Use Kafka CLI tools
- Database: Connect using psql or your preferred DB client

## Configuration
Key configuration options in `.env`:
- Database credentials
- Kafka settings
- API configurations
- Airflow settings

## Troubleshooting
Common issues and solutions:
1. Kafka connection issues
   - Check if Kafka are running
   - Verify network connectivity
2. Database connection issues
   - Check credentials in .env
   - Verify PostgreSQL is running
3. API errors
   - Check logs using `docker logs api`
   - Verify model file exists

## Contributing
1. Fork the repository
2. Create a feature branch
3. Commit changes
4. Push to the branch
5. Create a pull request

## License
[Your chosen license]

## Contact
[Your contact information]

## Acknowledgments
- List any libraries, tools, or resources you used
- Credit any contributors or inspirations

---

**Note**: This is a template README. Update sections according to your specific implementation and requirements.
